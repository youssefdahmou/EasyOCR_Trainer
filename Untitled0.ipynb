{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtjuSDpyZ73q",
        "outputId": "4d51d9ac-bb79-4e0c-daf1-44079b67b9c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/PFE/easyOCR_finetuning/train"
      ],
      "metadata": {
        "id": "oYTjN61d4wLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I- Generate the data"
      ],
      "metadata": {
        "id": "cvuHMlpV4VNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import matplotlib.patches as patches\n",
        "import os\n",
        "import subprocess\n",
        "from concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor,as_completed\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from skimage.color import rgb2gray\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.io import imread\n",
        "from skimage.filters import threshold_sauvola\n",
        "from skimage.exposure import is_low_contrast\n",
        "from skimage.exposure import adjust_gamma\n",
        "from skimage.restoration import denoise_tv_chambolle\n",
        "os.environ['OMP_THREAD_LIMIT'] = '1'"
      ],
      "metadata": {
        "id": "1mzQ5A4Q4M2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_digits(length):\n",
        "    return ''.join([str(random.randint(0, 9)) for i  in range(length)])\n",
        "def create_begain_digits():\n",
        "    n  = random.randint(0, 3)\n",
        "    if n == 0 : return '0' + create_digits(2)\n",
        "    else  : return '.' + create_digits(2)\n",
        "\n",
        "def display_image(image):\n",
        "    plt.figure(figsize=(10, 30))\n",
        "    plt.imshow(image )\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def textsize(text, font):\n",
        "    im = Image.new(mode=\"P\", size=(0, 0))\n",
        "    draw = ImageDraw.Draw(im)\n",
        "    _, _, width, height = draw.textbbox((0, 0), text=text, font=font)\n",
        "    return width, height\n",
        "\n",
        "def draw_text_with_coordinates(text, font_path = \"/content/drive/MyDrive/PFE/fonts/LinotypeCmc-Seven.ttf\" , background_color=(184, 179, 185)):\n",
        "    width, height =  2000, 100\n",
        "    #background_color = (255, 255, 255)  # white\n",
        "    img = Image.new(\"RGB\", (width, height), background_color)\n",
        "    font_size = random.randint(35, 55)\n",
        "    font = ImageFont.truetype(font_path, font_size)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    coordinates = []\n",
        "    x = 0\n",
        "    for digit in text:\n",
        "        width, height = textsize(digit, font=font)\n",
        "        draw.text((x, 30), digit, font=font, fill=(0, 0, 0))\n",
        "        coordinates.append((x, 30, x + width, 30 + height))\n",
        "        x += width\n",
        "    display_image(img)\n",
        "    return  img ,  coordinates\n",
        "\n",
        "def invert_image(thresholded_image):\n",
        "    inverted_image = cv2.bitwise_not(thresholded_image)\n",
        "    return inverted_image\n",
        "\n",
        "def draw_bounding_boxes(image, coordinates):\n",
        "    \"\"\"\n",
        "    Draw bounding boxes on the image based on the provided coordinates.\n",
        "    \"\"\"\n",
        "    if isinstance(image, np.ndarray):\n",
        "        # Convert NumPy array to PIL Image\n",
        "        image = Image.fromarray(image)\n",
        "\n",
        "    draw = ImageDraw.Draw(image)\n",
        "    for coord in coordinates:\n",
        "        draw.rectangle(coord, outline=\"red\")  # You can change the color or other properties as needed\n",
        "    return image\n",
        "\n",
        "def add_gaussian_noise(image, mean=0, sigma=25):\n",
        "    \"\"\"\n",
        "    Adds Gaussian noise to the given image.\n",
        "    \"\"\"\n",
        "    row, col = image.shape\n",
        "    gauss = np.random.normal(mean, sigma, (row, col))\n",
        "    noisy_image = np.clip(image + gauss, 0, 255).astype(np.uint8)\n",
        "    return noisy_image\n",
        "\n",
        "def add_gaussian_noise(image, mean=-10, sigma=25):\n",
        "    \"\"\"\n",
        "    Adds Gaussian noise to the given image.\n",
        "    \"\"\"\n",
        "    row, col, ch = image.shape  # Get height, width, and number of channels\n",
        "    gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
        "    noisy_image = np.clip(image + gauss, 0, 255).astype(np.uint8)\n",
        "    return noisy_image\n",
        "\n",
        "def smooth_image(image):\n",
        "    # Split the image into its RGB channels\n",
        "    b, g, r = cv2.split(image)\n",
        "\n",
        "    # Threshold each color channel separately to get binary images\n",
        "    _, thresh_b = cv2.threshold(b, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "    _, thresh_g = cv2.threshold(g, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "    _, thresh_r = cv2.threshold(r, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "    # Perform morphological closing on each binary image\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 1))\n",
        "    closing_b = cv2.morphologyEx(thresh_b, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "    closing_g = cv2.morphologyEx(thresh_g, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "    closing_r = cv2.morphologyEx(thresh_r, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
        "\n",
        "    # Invert each binary image\n",
        "    smoothed_b = cv2.bitwise_not(closing_b)\n",
        "    smoothed_g = cv2.bitwise_not(closing_g)\n",
        "    smoothed_r = cv2.bitwise_not(closing_r)\n",
        "\n",
        "    # Merge the smoothed color channels back into an RGB image\n",
        "    smoothed_image = cv2.merge((smoothed_b, smoothed_g, smoothed_r))\n",
        "\n",
        "    return smoothed_image\n",
        "\n",
        "def save_data(image, coordinates , folder ):\n",
        "\n",
        "    try :\n",
        "        files = os.listdir(folder)\n",
        "        files = [file for file in files if file.endswith('.tif')]\n",
        "        indices = [int(file.split('_')[1].split('.')[0]) for file in files]\n",
        "        i = max(indices) +1\n",
        "    except : i=0\n",
        "    # Save text to file\n",
        "    with open(os.path.join(folder, f'digi_{i}.gt.txt'), 'w') as text_file:\n",
        "        text_file.write(text)\n",
        "\n",
        "    # Save image\n",
        "    Image.fromarray(invert_image(np.array(image))).save(os.path.join(folder, f'digi_{i}.tif'))\n",
        "\n",
        "    # Save coordinates\n",
        "    with open(os.path.join(folder, f'digi_{i}.box'), 'w') as coordinates_file:\n",
        "        for coord in coordinates:\n",
        "            coord = coord = list(text[coordinates.index(coord)]) + list(coord) + [0]\n",
        "            coordinates_file.write(' '.join(map(str, coord)) + '\\n')\n",
        "    display_image(draw_bounding_boxes(image, coordinates))"
      ],
      "metadata": {
        "id": "MJ9Vp6Oo4M65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pil_to_cv(pil_image):\n",
        "    cv_image = np.array(pil_image)\n",
        "    cv_image = cv_image[:, :, ::-1].copy()\n",
        "    return cv_image\n",
        "\n",
        "def cv_to_pil(cv_img):\n",
        "    # return Image.fromarray(img)\n",
        "    return Image.fromarray(cv2.cvtColor(cv_img, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "def add_random_noise(image):\n",
        "    row, col, ch = image.shape\n",
        "    mean = -1\n",
        "    var = 0.5\n",
        "    sigma = var ** 0.5\n",
        "    gauss = np.random.normal(mean, sigma, (row, col, ch))\n",
        "    gauss = gauss.reshape(row, col, ch)\n",
        "    noisy_image = image + gauss\n",
        "    return noisy_image.astype('uint8')\n",
        "\n",
        "def apply_gaussian_blur(image, kernel_size=5):\n",
        "    blurred_image = cv2.GaussianBlur(image, (kernel_size, kernel_size), 0)\n",
        "    return blurred_image\n",
        "\n",
        "def apply_motion_blur(image, kernel_size=5):\n",
        "    kernel = np.zeros((kernel_size, kernel_size))\n",
        "    kernel[int((kernel_size - 1)/2), :] = np.ones(kernel_size)\n",
        "    kernel = kernel / kernel_size\n",
        "    blurred_image = cv2.filter2D(image, -1, kernel)\n",
        "    return blurred_image\n",
        "\n",
        "def apply_perspective_transform_with_boxes(image, boxes, delta=2):\n",
        "    rows, cols, ch = image.shape\n",
        "    pts1 = np.float32([[50, 50], [200, 50], [50, 200], [200, 200]])\n",
        "    pts2 = np.float32([\n",
        "        [50 + np.random.randint(-delta, delta), 50 + np.random.randint(-delta, delta)],\n",
        "        [200 + np.random.randint(-delta, delta), 50 + np.random.randint(-delta, delta)],\n",
        "        [50 + np.random.randint(-delta, delta), 200 + np.random.randint(-delta, delta)],\n",
        "        [200 + np.random.randint(-delta, delta), 200 + np.random.randint(-delta, delta)]\n",
        "    ])\n",
        "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
        "    transformed_image = cv2.warpPerspective(image, M, (cols, rows))\n",
        "\n",
        "    transformed_boxes = []\n",
        "    for box in boxes:\n",
        "        # Assuming box is a tuple (x1, y1, x2, y2)\n",
        "        pts = np.float32([\n",
        "            [box[0], box[1]],\n",
        "            [box[2], box[1]],\n",
        "            [box[0], box[3]],\n",
        "            [box[2], box[3]]\n",
        "        ]).reshape(-1, 1, 2)\n",
        "\n",
        "        # Apply the transformation to each corner of the bounding box\n",
        "        transformed_pts = cv2.perspectiveTransform(pts, M)\n",
        "\n",
        "        # Recalculate the bounding box as the min/max x and y coordinates of the transformed points\n",
        "        x_coords = transformed_pts[:, 0, 0]\n",
        "        y_coords = transformed_pts[:, 0, 1]\n",
        "        new_box = (min(x_coords), min(y_coords), max(x_coords), max(y_coords))\n",
        "        transformed_boxes.append(new_box)\n",
        "\n",
        "    return transformed_image, transformed_boxes\n",
        "\n",
        "def adjust_brightness_contrast(image, brightness=0, contrast=100):\n",
        "    new_image = np.zeros(image.shape, image.dtype)\n",
        "\n",
        "    alpha = np.random.randint(1,5)\n",
        "    beta = np.random.randint(-10,10)\n",
        "    # print(alpha,beta)\n",
        "    # Adjust brightness and contrast\n",
        "    new_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
        "    return new_image\n",
        "\n",
        "def display_bboxes(image, bounding_boxes):\n",
        "    image_with_boxes = np.copy(image)\n",
        "    for box in bounding_boxes:\n",
        "        x1, y1, x2, y2 = box\n",
        "        cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "    display_image(image_with_boxes)\n"
      ],
      "metadata": {
        "id": "V7A7m2mv4M_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder = 'data/val'\n",
        "#if not os.path.exists(folder):\n",
        "#    os.makedirs(folder)\n",
        "if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "\n",
        "# List all files in the folder\n",
        "files = os.listdir(folder)\n",
        "len(files)"
      ],
      "metadata": {
        "id": "oilHuY0I4mOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(folder):\n",
        "    os.makedirs(folder)\n",
        "\n",
        "def display_image(image,cmap='gray'):\n",
        "    plt.figure(figsize=(10, 30))\n",
        "    plt.imshow(image,cmap=cmap )\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "labels = ''\n",
        "img_id = 0\n",
        "\n",
        "for i in range(80):\n",
        "    # smooth image\n",
        "\n",
        "    img_id +=1\n",
        "\n",
        "    text = ''.join([create_begain_digits(), '.', create_digits(7), ';', create_digits(6), ';', create_digits(16), ';', create_digits(2), '-'])\n",
        "    image, coordinates = draw_text_with_coordinates(text,background_color = (200, 200, 200) )\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    image.save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    img_id +=1\n",
        "    print('smoth Denoised ')\n",
        "    dst = cv2.fastNlMeansDenoisingColored(np.array(image), None, 10, 10, 7, 15)\n",
        "    display_image(dst)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(dst).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    img_id +=1\n",
        "    print('erosion 1')\n",
        "    kernel = np.ones((1,3),np.uint8)\n",
        "    erosion = cv2.erode(dst,kernel,iterations = 1)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(erosion).save(image_path)\n",
        "    display_image(cv_to_pil(erosion))\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    img_id +=1\n",
        "    print('binarized 1' ,  img_id)\n",
        "    gamma_corrected = adjust_gamma(erosion, 1.2)\n",
        "    gry_img = rgb2gray(gamma_corrected)\n",
        "    th = threshold_sauvola(gry_img, 255)\n",
        "    bimage = (gry_img > th).astype(int)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    bimage = np.array(bimage).astype(np.uint8)\n",
        "    display_image(bimage)\n",
        "    #Image.fromarray(bimage).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "    cv2.imwrite(image_path, bimage*255 )\n",
        "\n",
        "    img_id +=1\n",
        "    print('erosion 2')\n",
        "    kernel = np.ones((1,3),np.uint8)\n",
        "    erosion = cv2.erode(dst,kernel,iterations = 2)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(erosion).save(image_path)\n",
        "    display_image(cv_to_pil(erosion))\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "\n",
        "\n",
        "    img_id +=1\n",
        "    print('binarized 2 ')\n",
        "    gamma_corrected = adjust_gamma(erosion, 1.2)\n",
        "    gry_img = rgb2gray(gamma_corrected)\n",
        "    th = threshold_sauvola(gry_img, 255)\n",
        "    bimage = (gry_img > th).astype(int)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    bimage = np.array(bimage).astype(np.uint8)\n",
        "    display_image(bimage)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "    cv2.imwrite(image_path, bimage*255 )\n",
        "\n",
        "    img_id +=1\n",
        "    print('erosion 2')\n",
        "    kernel = np.ones((1,3),np.uint8)\n",
        "    erosion = cv2.erode(dst,kernel,iterations = 3)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(erosion).save(image_path)\n",
        "    display_image(cv_to_pil(erosion))\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "\n",
        "\n",
        "    img_id +=1\n",
        "    print('binarized 2 ')\n",
        "    gamma_corrected = adjust_gamma(erosion, 1.2)\n",
        "    gry_img = rgb2gray(gamma_corrected)\n",
        "    th = threshold_sauvola(gry_img, 255)\n",
        "    bimage = (gry_img > th).astype(int)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    bimage = np.array(bimage).astype(np.uint8)\n",
        "    display_image(bimage)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "    cv2.imwrite(image_path, bimage*255 )\n",
        "\n",
        "    img_id +=1\n",
        "    print('noised')\n",
        "    text = ''.join([create_begain_digits(), '.', create_digits(7), ';', create_digits(6), ';', create_digits(16), ';', create_digits(2), '-'])\n",
        "    image, coordinates = draw_text_with_coordinates(text,background_color = (200, 200, 200) )\n",
        "\n",
        "    noised = add_random_noise(np.array(image))\n",
        "    display_image(noised)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(noised).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    img_id +=1\n",
        "    print('Blur')\n",
        "    blurred = apply_motion_blur(noised)\n",
        "    display_image(blurred)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(blurred).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    print(' gray ')\n",
        "    img_id +=1\n",
        "    gray = cv2.cvtColor(pil_to_cv(image), cv2.COLOR_BGR2GRAY)\n",
        "    display_image(gray)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(gray).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    print(' dilated ')\n",
        "    img_id +=1\n",
        "    kernel = np.ones((3,1), np.uint8)# cv2.getStructuringElement(cv2.MORPH_RECT, (5,1))\n",
        "    dilated = cv2.dilate(gray, kernel, iterations=2)\n",
        "    display_image(dilated )\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(dilated).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    print(' thresh')\n",
        "    img_id +=1\n",
        "    _, thresh = cv2.threshold(gray, 230, 255, cv2.THRESH_OTSU )\n",
        "    display_image(thresh)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(thresh).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "\n",
        "    #Distort  ###################\n",
        "    text = ''.join([create_begain_digits(), '.', create_digits(7), ';', create_digits(6), ';', create_digits(16), ';', create_digits(2), '-'])\n",
        "    image, coordinates = draw_text_with_coordinates(text,background_color = (200, 200, 200) )\n",
        "\n",
        "    img_id +=1\n",
        "    print('distorted ')\n",
        "    distorted, distorted_boxes = apply_perspective_transform_with_boxes(np.array(image),coordinates , 1 )\n",
        "    distorted_boxes = [ (int(cord[0]), int(cord[1]), int(cord[2]), int(cord[3])) for cord in   distorted_boxes]\n",
        "    display_image(distorted)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(distorted).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    print('dis_gray ')\n",
        "    img_id +=1\n",
        "    dis_gray = cv2.cvtColor(distorted, cv2.COLOR_BGR2GRAY)\n",
        "    display_image(dis_gray)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(dis_gray).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    print(' dis_dilated ')\n",
        "    img_id +=1\n",
        "    kernel = np.ones((1,3),np.uint8)\n",
        "    dis_dilated = cv2.erode(dis_gray,kernel,iterations = 2)\n",
        "    display_image(dis_dilated)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(dis_dilated ).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    print(' dis_eroded')\n",
        "    img_id +=1\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1,3))\n",
        "    dis_eroded = cv2.dilate(dis_dilated, kernel, iterations=3)\n",
        "    display_image(dis_eroded)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(dis_eroded ).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'\n",
        "\n",
        "    print(' dis_thresh ')\n",
        "    img_id +=1\n",
        "    _, dis_thresh = cv2.threshold(dis_gray, 230, 255, cv2.THRESH_OTSU)\n",
        "    display_image(dis_thresh)\n",
        "    image_path = os.path.join(folder, '{}.png'.format(img_id))\n",
        "    cv_to_pil(dis_thresh).save(image_path)\n",
        "    labels += f'{img_id}.png {text}\\n'"
      ],
      "metadata": {
        "id": "JSd1hWFU4mZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path =  '/content/drive/MyDrive/PFE/easyOCR_finetuning/EasyOCR/trainer/all_data/labels.txt'\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "      file.write(l)"
      ],
      "metadata": {
        "id": "lz2tQSw84NDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II - Instalation of the Required Packages"
      ],
      "metadata": {
        "id": "2B1qCZLZ57la"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIw_oawuZlEy",
        "outputId": "2b219122-b3c7-4266-b652-0ee50fbe4ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire) (2.4.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=243c8dcd764a815a7e6ff3b20d230cf53dd732fa133121c6ec2cd6a29c65fe19\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.6.0\n",
            "Collecting lmdb\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lmdb\n",
            "Successfully installed lmdb-1.4.1\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (8.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install fire\n",
        "!pip install lmdb\n",
        "!pip install opencv-python\n",
        "!pip install natsort\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLt4HZQuZzH7",
        "outputId": "6cc4b846-386f-4677-ad39-df1361fba1f4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42mUvyCUa_Ik",
        "outputId": "6e3f0f41-a176-46d7-ccd7-938e096d3811"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.2.1%2Bcu118-cp310-cp310-linux_x86_64.whl (819.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.2/819.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.7.0.84 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.19.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl (135.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 torch-2.2.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/clovaai/deep-text-recognition-benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYzilolRbATm",
        "outputId": "bcb6181d-2660-4554-81a1-1afa414d05fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'deep-text-recognition-benchmark'...\n",
            "remote: Enumerating objects: 499, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 499 (delta 0), reused 1 (delta 0), pack-reused 495\u001b[K\n",
            "Receiving objects: 100% (499/499), 3.07 MiB | 9.62 MiB/s, done.\n",
            "Resolving deltas: 100% (301/301), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd deep-text-recognition-benchmark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAE9K_V3gI2D",
        "outputId": "48f4076c-46fe-4b5e-eac3-1b68d6fea298"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'deep-text-recognition-benchmark'\n",
            "/content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "BpwNNsFx6Pl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "550c4851-2708-46dc-aa09-c4c496cd7497"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III -  Convert the Dataset to LMDB Format\n",
        "> ## 1) - trainSet"
      ],
      "metadata": {
        "id": "SBlABnXc7Xu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python create_lmdb_dataset.py /content/drive/MyDrive/PFE/easyOCR_finetuning/EasyOCR/trainer/all_data/en_filtered /content/drive/MyDrive/PFE/easyOCR_finetuning/EasyOCR/trainer/all_data/en_filtered/labels.txt lmbd_output/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIelpkfQeesy",
        "outputId": "547328c1-39d3-4742-f5ef-889cc335b890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written 1000 / 3600\n",
            "Written 2000 / 3600\n",
            "Written 3000 / 3600\n",
            "Created dataset with 3600 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ## 2) - ValSet"
      ],
      "metadata": {
        "id": "-xU3LZK47j-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python create_lmdb_dataset.py /content/drive/MyDrive/PFE/easyOCR_finetuning/EasyOCR/trainer/all_data/val /content/drive/MyDrive/PFE/easyOCR_finetuning/EasyOCR/trainer/all_data/val/labels.txt lmbd_output/val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg69_vmRuJ7w",
        "outputId": "c4dd0d19-68a5-4a43-fc9b-51e3c5452f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written 1000 / 1440\n",
            "Created dataset with 1440 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IV - the Fine-tuning"
      ],
      "metadata": {
        "id": "gs6uJV807yVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6NXM2GffmBh",
        "outputId": "910a1e40-d682-4356-a616-7c19103786fa"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
            "Collecting torch==1.13.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m662.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1+cu117\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchtext==0.14.1\n",
            "  Downloading torchtext-0.14.1-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.13.1\n",
            "  Downloading https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchdata==0.5.1\n",
            "  Downloading torchdata-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.1+cu117) (4.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (1.25.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.14.1+cu117) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.14.1) (4.66.2)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.5.1) (2.0.7)\n",
            "Collecting portalocker>=2.0.0 (from torchdata==0.5.1)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.14.1+cu117) (2024.2.2)\n",
            "Installing collected packages: torch, portalocker, torchvision, torchtext, torchdata, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu118\n",
            "    Uninstalling torch-2.2.1+cu118:\n",
            "      Successfully uninstalled torch-2.2.1+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.17.1+cu121\n",
            "    Uninstalling torchvision-0.17.1+cu121:\n",
            "      Successfully uninstalled torchvision-0.17.1+cu121\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.17.1\n",
            "    Uninstalling torchtext-0.17.1:\n",
            "      Successfully uninstalled torchtext-0.17.1\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.7.1\n",
            "    Uninstalling torchdata-0.7.1:\n",
            "      Successfully uninstalled torchdata-0.7.1\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.2.1+cu121\n",
            "    Uninstalling torchaudio-2.2.1+cu121:\n",
            "      Successfully uninstalled torchaudio-2.2.1+cu121\n",
            "Successfully installed portalocker-2.8.2 torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchdata-0.5.1 torchtext-0.14.1 torchvision-0.14.1+cu117\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python conda install pytorch torchvision torchaudio pytorch-cuda=11.7 -c pytorch -c nvidia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_pvMOGBgU2Z",
        "outputId": "2268a65d-eb2d-45bc-ea94-5b09cc6aa0f6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark/conda': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --train_data lmdb_output/train --valid_data lmdb_output/train --data_filtering_off --select_data \"/\" --batch_ratio 1.0 --Transformation None --FeatureExtraction VGG --SequenceModeling None --Prediction CTC --batch_size 62 --data_filtering_off --workers 0 --batch_max_length 80 --num_iter 100 --valInterval 5 --saved_model pre_trained_models/None-VGG-None-CTC.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4u45lGrhFn8",
        "outputId": "7f348bcc-0311-413a-ba3f-82d65e0577cb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "dataset_root: lmdb_output/train\n",
            "opt.select_data: ['/']\n",
            "opt.batch_ratio: ['1.0']\n",
            "--------------------------------------------------------------------------------\n",
            "dataset_root:    lmdb_output/train\t dataset: /\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark/train.py\", line 331, in <module>\n",
            "    train(opt)\n",
            "  File \"/content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark/train.py\", line 31, in train\n",
            "    train_dataset = Batch_Balanced_Dataset(opt)\n",
            "  File \"/content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark/dataset.py\", line 42, in __init__\n",
            "    _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
            "  File \"/content/drive/MyDrive/PFE/Easy_OCR/train/deep-text-recognition-benchmark/dataset.py\", line 124, in hierarchical_dataset\n",
            "    concatenated_dataset = ConcatDataset(dataset_list)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 317, in __init__\n",
            "    assert len(self.datasets) > 0, 'datasets should not be an empty iterable'  # type: ignore[arg-type]\n",
            "AssertionError: datasets should not be an empty iterable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python train.py --train_data lmdb_output/train --valid_data lmdb_output/val --select_data \"/\" --batch_ratio 1.0 --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --batch_size 2 --data_filtering_off --workers 0 --batch_max_length 80 --num_iter 10 --valInterval 5 --saved_model pre_trained_models/TPS_ResNet_BiLSTM_Attn.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "Y5-ff8NAp0YD",
        "outputId": "12e2be55-cbc4-4810-e0f1-e73b76ce8149"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-17-09a6f40d1aa1>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-09a6f40d1aa1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python train.py --train_data lmdb_output/train --valid_data lmdb_output/val --select_data \"/\" --batch_ratio 1.0 --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --batch_size 2 --data_filtering_off --workers 0 --batch_max_length 80 --num_iter 10 --valInterval 5 --saved_model pre_trained_models/TPS_ResNet_BiLSTM_Attn.pth\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hs1r36iTh-zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8LxBJf6h-1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4B-y72i0h-4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M8LGz1GqkAjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}